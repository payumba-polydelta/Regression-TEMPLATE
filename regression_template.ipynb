{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary to display the plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ydata_profiling import ProfileReport\n",
    "from IPython.display import display_html, display_markdown, HTML, Markdown as md\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.lines import Line2D\n",
    "import math\n",
    "import re\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from joblib import dump\n",
    "import time\n",
    "from typing import Union\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import KFold,GridSearchCV, train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Style Settings and Display Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
        "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def display_dataframe(df: pd.DataFrame,\n",
    "                      font_size: int = 14) -> None:\n",
    "    \"\"\"\n",
    "    Displays the passed in DataFrame with the specified font size.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be displayed.\n",
    "        font_size (int): The font size at which the items in the\n",
    "        DataFrame should be displayed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df_html = df.to_html()\n",
    "    styled_html = f'<div style=\"font-size: {font_size}px;\">{df_html}</div>'\n",
    "    display_html(HTML(styled_html))\n",
    "    \n",
    "\n",
    "def display_text(text: str,\n",
    "                 font_size: int = 16,\n",
    "                 font_weight = 'normal') -> None:\n",
    "    \"\"\"\n",
    "    Displays the passed in text with the specified font size and font weight.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be displayed.\n",
    "        font_size (int): The font size at which the text should be displayed.\n",
    "        font_weight: The font weight (e.g., 'normal', 'bold', 'bolder', 'lighter',\n",
    "        or numeric value from 100 to 900).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    styled_html = f'<div style=\"font-size: {font_size}px; font-weight: {font_weight};\">{text}</div>'\n",
    "    display_html(HTML(styled_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    },
    "tags": [
        "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&display=swap');\n",
    "div.text_cell {\n",
    "    font-family : DM Sans, sans-serif !important;\n",
    "    font-size : 1.2em !important;\n",
    "}\n",
    "pre {font-family : DM Sans, sans-serif !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regression Model Comparison Template** \n",
    "This Notebook compares the performance of different types of regression models on a dataset provided by the user. It assists in the model selection process by streamlining data cleaning, data preprocessing, model training, and model evaluation. Throughout the template there are sections the user must configure to match characteristics of their dataset. These sections are preceeded by tripple quote comments that direct them on how to proceed. Users should store files containing their data in the data directory. The load_data function automatically prepends /data to the given file name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load and Reformat the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configure Data Loading Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input the name of the file containing your dataset, a list of the name of columns to drop, and a list of\n",
    "the representations of missing values in the dataset.\n",
    "\"\"\"\n",
    "data_file_name: str = \"(ex: data.csv)\" \n",
    "columns_to_drop: list[str] = [] \n",
    "\n",
    "# Ensure no valid dataset values are included in this list\n",
    "dataset_na_value_representations = ['', 'NA', 'N/A', 'null', 'NULL', 'NaN', 'none', 'None', '-', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Loading Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
        "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def load_data(file_name: str = data_file_name,\n",
    "              dropped_columns = columns_to_drop,\n",
    "              na_value_representations: list[str] = dataset_na_value_representations) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads in user's input file as a pandas DataFrame and converts various representations of missing\n",
    "    values to NaN. The file should be stored in the 'data' directory.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Name of file containing data for clustering\n",
    "        dropped_columns (list[str]): List of columns to drop from the dataframe\n",
    "        na_value_representations (list[str]): List of strings that represent missing values in\n",
    "            the dataset\n",
    "    Returns:\n",
    "        df (pd.DataFrame): Dataframe of variable values for all data entries\n",
    "    \"\"\"\n",
    "    # Automatically prepends 'data/' to the file name\n",
    "    file_name: str = \"data/\" + file_name\n",
    "    file_extension: str = file_name.split(\".\")[-1]\n",
    "\n",
    "    if file_extension == \"csv\":\n",
    "        df = pd.read_csv(file_name)\n",
    "    elif file_extension in [\"xls\", \"xlsx\"]:\n",
    "        if file_extension == \"xls\":\n",
    "            df = pd.read_excel(file_name, engine = 'xlrd')\n",
    "        else:\n",
    "            df = pd.read_excel(file_name, engine = 'openpyxl')\n",
    "    elif file_extension == \"json\":\n",
    "        df = pd.read_json(file_name)\n",
    "    else:\n",
    "        raise ValueError(\"\"\"Unsupported file format or misspelled file name. Please upload \n",
    "                         a CSV, Excel, or JSON file and ensure the file name is spelled correctly.\"\"\")\n",
    "    \n",
    "    # Replaces input representations of missing values with np.nan\n",
    "    df = df.replace(na_value_representations, np.nan)\n",
    "    \n",
    "    df = df.drop_duplicates()\n",
    "    df = df.drop(columns = dropped_columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df: pd.DataFrame = load_data()\n",
    "\n",
    "initial_number_of_entries: int = len(data_df) \n",
    "variable_list: list[str] = list(data_df.columns)\n",
    "number_of_variables: int = len(variable_list)\n",
    "\n",
    "numerical_variables: list[str] = list(data_df.select_dtypes(include = np.number).columns)\n",
    "categorical_variables: list[str] = list(data_df.select_dtypes(exclude = np.number).columns)\n",
    "\n",
    "display_text(f\"Numerical Variables: {numerical_variables}\", font_size = 16)\n",
    "display_text(f\"Categorical Variables: {categorical_variables}\", font_size = 16)\n",
    "\n",
    "display_dataframe(data_df.head(), font_size = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reformat Columns**\n",
    "Do not run this cell if the values of your dataset are already properly formatted. If not (e.g. columns that should be numerical are instead represented as strings), open the cell and configure this section to reformat any imporperly formatted columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_float(value_str: str):\n",
    "    \"\"\"\n",
    "    Cleans a string by removing all characters except digits and decimal points directly followed by a digit\n",
    "    using regular expressions and converts the cleaned string to a float. This function may not work as\n",
    "    intended with some strings that contain multiple and/or awkwardly placed decimal points.\n",
    "\n",
    "    Args:\n",
    "        value_str (str): The string to be cleaned and converted.\n",
    "\n",
    "    Returns:\n",
    "        float or None: The cleaned float value or None if conversion fails.\n",
    "    \"\"\"\n",
    "    # Check if the value is np.nan and returns None if it is\n",
    "    if pd.isna(value_str):\n",
    "        return None\n",
    "    \n",
    "    # Remove all characters except digits and decimal points followed by a digit.\n",
    "    cleaned_str: str = re.sub(r'[^0-9.]+', '', value_str)\n",
    "    \n",
    "    # Additional check to handle multiple decimal points or trailing decimal points\n",
    "    parts: list[str] = cleaned_str.split('.')\n",
    "    if len(parts) > 2:\n",
    "        cleaned_str: str = ''.join(parts[:-1]) + '.' + parts[-1]\n",
    "    elif len(parts) == 2 and parts[1] == '':\n",
    "        cleaned_str: str = parts[0]\n",
    "    \n",
    "    # Attempt to convert string to float\n",
    "    try:\n",
    "        float_value = pd.to_numeric(cleaned_str)\n",
    "    except ValueError:\n",
    "        print(f\"Failed to convert {value_str} to a float\")\n",
    "        # Sets the float_value to None if the string cannot be converted to a float\n",
    "        float_value = None\n",
    "    \n",
    "    return float_value\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Apply value cleaning/conversion functions to the relevant columns of your datset using the apply \n",
    "method on your DataFrame. Your cleaing/conversion functions should take in a single string and return a float.\n",
    "Pass them into the apply method as an argument without parentheses.\n",
    "Example: data_df['column_name'] = data_df['column_name'].apply(clean_function).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "display_text(f\"Previous List of Numerical Variables: {numerical_variables}\", font_size = 18)\n",
    "display_text(f\"Previous List of Categorical Variables: {categorical_variables}\", font_size = 18)\n",
    "print()\n",
    "\n",
    "numerical_variables: list[str] = list(data_df.select_dtypes(include = np.number).columns)\n",
    "categorical_variables: list[str] = list(data_df.select_dtypes(exclude = np.number).columns)\n",
    "\n",
    "display_text(f\"Updated List of Numerical Variables: {numerical_variables}\", font_size = 18)\n",
    "display_text(f\"Updated List of Categorical Variables: {categorical_variables}\", font_size = 18)\n",
    "\n",
    "display_dataframe(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handle Missing Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initial Data Profiling**\n",
    "The following cell uses ydata-profiling to generate a detailed report on the characteristics of the input dataset. Use this information to help you determine how to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[categorical_variables] = data_df[categorical_variables].astype(\"category\")\n",
    "\n",
    "initial_dataset_report = ProfileReport(data_df, title = \"Dataset Profiling Report (Before Handling Outliers/Missing Values)\", progress_bar = False, explorative = True)\n",
    "initial_dataset_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Information on Missing Values in Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_with_missing_values: list[str] = data_df[numerical_variables].columns[data_df[numerical_variables].isnull().any()].tolist()\n",
    "categorical_columns_with_missing_values: list[str] = data_df[categorical_variables].columns[data_df[categorical_variables].isnull().any()].tolist()\n",
    "all_columns_with_missing_values: list[str] = numerical_columns_with_missing_values + categorical_columns_with_missing_values\n",
    "\n",
    "if len(all_columns_with_missing_values) != 0:\n",
    "    print()\n",
    "    entries_with_missing_values_df: pd.DataFrame = data_df[all_columns_with_missing_values][data_df[all_columns_with_missing_values].isnull().any(axis = \"columns\")]\n",
    "    number_of_entries_with_missing_values: int = len(entries_with_missing_values_df)\n",
    "    percent_of_entries_with_missing_values: float = (number_of_entries_with_missing_values / initial_number_of_entries) * 100  \n",
    "    \n",
    "    display_text(f\"Total Number of Entries: {initial_number_of_entries}\")\n",
    "    display_text(f\"Total Number of Entreis with at Least One Missing Value: {number_of_entries_with_missing_values} ({percent_of_entries_with_missing_values:.2f}% of Entries)\")\n",
    "    display_text(f\"Number of Entries if all Rows with Missing Values are Dropped: {initial_number_of_entries - number_of_entries_with_missing_values}\")\n",
    "    print()\n",
    "    display_text(\"Up to First 5 Entries with Missing Values:\")\n",
    "    display_dataframe(entries_with_missing_values_df.head(), font_size = 16)\n",
    "else:\n",
    "    print()\n",
    "    display_text(\"No Missing Values in Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Functions for Dropping or Imputing Mising Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop_rows_with_missing_values\u001b[39m(df: pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m=\u001b[39m \u001b[43mdata_df\u001b[49m,\n\u001b[1;32m      2\u001b[0m                                   columns_to_check: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m all_columns_with_missing_values) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Makes a copy of the input DataFrame and drops rows that have one or more missing values in any of the columns specified by \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    the columns_to_check parameter (does not mutate the input DataFrame). Also prints the number of entries dropped and the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m        dropna_df (pd.DataFrame): DataFrame with missing values dropped\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     original_number_of_entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "def drop_rows_with_missing_values(df: pd.DataFrame = data_df,\n",
    "                                  columns_to_check: list[str] = all_columns_with_missing_values) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Makes a copy of the input DataFrame and drops rows that have one or more missing values in any of the columns specified by \n",
    "    the columns_to_check parameter (does not mutate the input DataFrame). Also prints the number of entries dropped and the\n",
    "    resulting total number of entries.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing loded in data\n",
    "        columns_to_check (list[str]): List of columns to check for missing values\n",
    "    Returns:\n",
    "        dropna_df (pd.DataFrame): DataFrame with missing values dropped\n",
    "    \"\"\"\n",
    "    \n",
    "    original_number_of_entries = len(df)\n",
    "    \n",
    "    dropna_df = df.dropna(subset = columns_to_check)\n",
    "    new_number_of_entries = len(dropna_df)\n",
    "    number_of_entries_dropped = original_number_of_entries - new_number_of_entries\n",
    "    \n",
    "    display_text(f\"drop_rows_with_missing_values Results: {number_of_entries_dropped} Entries Dropped\") \n",
    "    display_text(f\"New Number of Entries: {new_number_of_entries}\")\n",
    "    \n",
    "    return dropna_df\n",
    "\n",
    "\n",
    "def impute_missing_values(df: pd.DataFrame = data_df,\n",
    "                          numerical_columns_to_impute: list[str] = numerical_columns_with_missing_values,\n",
    "                          categorical_columns_to_impute: list[str] = categorical_columns_with_missing_values) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputes missing values in the DataFrame with either the median value (for numerical variables) or the most frequent value\n",
    "    (for categorical variables).\n",
    "    \n",
    "    Args:\n",
    "        numerical_columns_to_impute (list[str]): List of the names of numerical columns with missing values to impute\n",
    "        categorical_columns_to_impute (list[str]): List of the names of categorical columns with missing values to impute\n",
    "    Returns:\n",
    "        impute_df (pd.DataFrame): DataFrame with missing values imputed\n",
    "    \"\"\"\n",
    "    impute_df = df.copy()\n",
    "    \n",
    "    # Here is where to configure the imputation strategy if need be\n",
    "    numerical_imputer = SimpleImputer(strategy = \"median\")\n",
    "    categorical_imputer = SimpleImputer(strategy = \"most_frequent\")\n",
    "    \n",
    "    impute_df[numerical_columns_to_impute] = numerical_imputer.fit_transform(impute_df[numerical_columns_to_impute])\n",
    "    impute_df[categorical_columns_to_impute] = categorical_imputer.fit_transform(impute_df[categorical_columns_to_impute])\n",
    "    \n",
    "    display_text(\"Missing Values Successfully Imputed\")\n",
    "    \n",
    "    return impute_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Drop or Impute Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Specify how you would like to handle missing values in the dataset. All rows with missing data are dropped by default. Please work\n",
    "\"\"\"\n",
    "data_df = drop_rows_with_missing_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handle Outliers/Eronious Entries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Outlier Vizualization Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_outliers(df: pd.DataFrame = data_df,\n",
    "                       numerical_columns_to_check: Union[list[str], str] = numerical_variables,\n",
    "                       iqr_multiplier: float = 1.5,\n",
    "                       remove: bool = False,\n",
    "                       remove_option: str = 'both',\n",
    "                       display: bool = True,) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a boxplot for each column of the input Dataframe in the numerical_columns_to_check parameter to help users visualize potential\n",
    "    outliers in their dataset. Below this boxplot, the function prints the number of high and low outliers (determined by the IQR method) in the\n",
    "    current column. The upper and lower bounds for outliers are denoted by red dotted lines. Points below the low bound red dotted line\n",
    "    or above the high bound red dotted line are consideered outliers. Users can choose whether to drop outlier entries through the remove\n",
    "    boolean parameter. You can change which points are considered outliers by changing the iqr_multiplier parameter.\n",
    "    \n",
    "    The lower and upper whiskers of the boxplot denote the 5th and 95th percentile of the current column's values respectively.\n",
    "\n",
    "    This function can be used iteratively to handle outliers in different columns with varying sensitivity levels. It allows for\n",
    "    selective removal of entries below/above the red dotted lines. The function can be run without displaying visualizations for efficiency.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data to be analyzed.\n",
    "        numerical_columns_to_check (Union[list[str], str], default = numerical_variables): List of the names of columns to check for outliers. The\n",
    "            default argument is a list of all numerical columns in the input DataFrame.\n",
    "        iqr_multiplier (float, default = 1.5): Multiplier for the IQR to define the outlier threshold. Higher values are more lenient, increasing\n",
    "            the range of the upper and lower red dotted lines. Lower values are more strict, decreasing the range of the red dotted lines.\n",
    "        remove (bool, default = False): If True, removes identified outliers from the DataFrame.\n",
    "        remove_option (str, default = 'both'): Specifies which outliers to remove: 'both' removes all identified outliers, 'upper' only removes\n",
    "            outliers greater than the upper bound (values past the upper red dotted line), and 'lower' only removes outliers less than the\n",
    "            lower bound (values behind the lower red dotted line). This parameter has no effect if remove = False.\n",
    "        display (bool, default = True): If True, displays boxplots for each variable. If false, only outlier statistics are printed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame if remove = False, otherwise a new DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    # If a single column is passed in as a string, convert it to a list so the following for loop still works properly\n",
    "    if type(numerical_columns_to_check) == str:\n",
    "        numerical_columns_to_check = [numerical_columns_to_check]\n",
    "        \n",
    "    for col in numerical_columns_to_check:\n",
    "        q1 = df[col].quantile(0.25)\n",
    "        q3 = df[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - (iqr * iqr_multiplier)\n",
    "        upper_bound = q3 + (iqr * iqr_multiplier)\n",
    "        \n",
    "        # Only create plot if display is True\n",
    "        if display:\n",
    "            plt.figure(figsize = (10, 6))\n",
    "            ax = sns.boxplot(x = df[col], whis = [5, 95])\n",
    "            plt.title(f'Boxplot of {col}')\n",
    "            \n",
    "            # Add vertical red dotted lines for lower and upper bounds if within the plot's x-axis limits\n",
    "            x_min, x_max = ax.get_xlim()\n",
    "            if x_min <= lower_bound <= x_max:\n",
    "                plt.axvline(lower_bound, color='red', linestyle='dotted', linewidth=1)\n",
    "            if x_min <= upper_bound <= x_max:\n",
    "                plt.axvline(upper_bound, color='red', linestyle='dotted', linewidth=1)\n",
    "            \n",
    "            # Create legend\n",
    "            legend_lines = [Line2D([0], [0], color='red', linestyle='dotted', linewidth=1)]\n",
    "            legend_labels = ['Lower/Upper Bound']\n",
    "            plt.legend(legend_lines, legend_labels, loc='upper right')\n",
    "            plt.show()\n",
    "        \n",
    "        lower_outlier_count = df[col][df[col] < lower_bound].count()\n",
    "        upper_outlier_count = df[col][df[col] > upper_bound].count()\n",
    "        \n",
    "        display_text(f\"{col}:\", font_size = 18, font_weight = 'bold')\n",
    "        display_text(f\"- Lower Bound for Outliers: {lower_bound}\", font_size = 16)\n",
    "        display_text(f\"- Upper Bound for Outliers: {upper_bound}\", font_size = 16)\n",
    "        display_text(f\"- Number of Outliers Below Lower Bound: {lower_outlier_count}\", font_size = 16)\n",
    "        display_text(f\"- Number of Outliers Above Upper Bound: {upper_outlier_count}\", font_size = 16)\n",
    "        print()\n",
    "        \n",
    "    # Removes outliers from the DataFrame if remove = True\n",
    "    if remove:\n",
    "        # Calculate indices of outliers\n",
    "        lower_outlier_indices = df.index[df[col] < lower_bound].tolist()\n",
    "        upper_outlier_indices = df.index[df[col] > upper_bound].tolist()\n",
    "        outlier_indices_to_be_removed = set()\n",
    "        \n",
    "        # Add outlier indices that will be removed to outlier_indices_to_be_removed based on the remove_option parameter\n",
    "        if remove_option == \"both\":\n",
    "            outlier_indices_to_be_removed.update(lower_outlier_indices)\n",
    "            outlier_indices_to_be_removed.update(upper_outlier_indices)\n",
    "        elif remove_option == \"lower\":\n",
    "            outlier_indices_to_be_removed.update(lower_outlier_indices)\n",
    "        elif remove_option == \"upper\":\n",
    "            outlier_indices_to_be_removed.update(upper_outlier_indices)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid argument passed into remove_option parameter. Please use 'both', 'lower', or 'upper'.\")\n",
    "            \n",
    "        removed_outliers_df = df.drop(index = outlier_indices_to_be_removed)\n",
    "        display_text(f\"Total Number of Outlier Entries Removed in {col}: {len(outlier_indices_to_be_removed)}\", font_size = 18)\n",
    "        print()\n",
    "        return removed_outliers_df\n",
    "    \n",
    "    # Simply return the original DataFrame if remove = False\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implement Outlier Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides information on variable distributions to help users determine whether they should drop outlier entries\n",
    "display_text(\"Previous Numerical Variable Statistics\", font_size = 20)\n",
    "display_dataframe(data_df.describe(), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the visualize_outliers function to identify and optionally remove outliers in the numerical columns of your dataset.\n",
    "\"\"\"\n",
    "data_df = visualize_outliers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text(\"Updated Numerical Variable Statistics\", font_size = 20)\n",
    "display_dataframe(data_df.describe(), 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**\n",
    "Define your preprocessing steps within this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Preprocessing Information**\n",
    "Use the information provided by the comparison profile report to analyze the result of your data cleaning and to help determine your preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_report = ProfileReport(data_df, title = \"Dataset Profiling Report (After Handling Outliers/Missing Values)\", progress_bar = False, explorative = True)\n",
    "# The difference between the profiling report before and after preprocessing may not be very significant depending on the number of missing values\n",
    "# and removed outliers\n",
    "post_cleaning_comparison_report = dataset_report.compare(initial_dataset_report)\n",
    "\n",
    "#post_cleaning_comparison_report\n",
    "dataset_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configure Preprocessing Steps for the Provided General Preprocessor**\n",
    "The provided general preprocessor address three common preprocessing transformations: scaling numerical variables, one-hot encoding nominal categorical variables, and ordianal encoding ordinal categorical variables. Customize these steps to fit the needs of your dataset. Preprocessing for your target variable and feature variables must be handled by seperate transformer variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input the name of the target variable column, a list of the numerical features you would like to scale (defaultes to all numerical\n",
    "features in the dataset), and a list of the nominal categorical features you would like to one-hot encode.\n",
    "\"\"\"\n",
    "target_column_name: str = \"\"\n",
    "numerical_features_to_scale: list[str] = list(set(numerical_variables) - set([target_column_name]))\n",
    "nominal_categorical_features_to_encode: list[str] = []\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The ordianl_categories_ordered_dict variable represents the order of ordinal categorical variable categories in a dictionary.\n",
    "For the keys of this dictionary, input the column names of ordinal categorical variables. Each key's value should be a lists\n",
    "of variable categories ordered from \"smallest\" to \"largest\". Example:\n",
    "ordianl_categories_ordered_dict = {\n",
    "    \"size\": [\"small\", \"medium\", \"large\"],\n",
    "    \"grade\": [\"F\", \"D\", \"C\", \"B\", \"A]\n",
    "}\n",
    "\"\"\"\n",
    "ordianl_categories_ordered_dict: dict[str, list[str]] = {}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Specify the preprocessor used on the target variable column. Default is StandardScaler.\n",
    "\"\"\"\n",
    "numerical_target_preprocessor = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **General Preprocessor Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the keys and values of the ordianl_categories_ordered_dict into separate lists\n",
    "ordianl_feature_categories_to_encode: list[str] = list(ordianl_categories_ordered_dict.keys())\n",
    "ordianl_feature_categories_orders_lists: list[list[str]] = list(ordianl_categories_ordered_dict.values())\n",
    "\n",
    "# Indicates that the first column of one-hot encoded variables should be dropped to avoid multicollinearity\n",
    "onehot_drop_column: str = \"first\"\n",
    "\n",
    "# The argumnet for the transformers parameter of ColumnTransformer must be a a list of touples with three entries. Each of these touples\n",
    "# represents a preprocessing step. The first entry of each touple is a name for the step. The second entry is the transformer object, and\n",
    "# the final entry is a list of the columns the step should be applied to.\n",
    "general_feature_preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('numerical_scaler', StandardScaler(), numerical_features_to_scale),\n",
    "        ('nominal_encoder', OneHotEncoder(drop = onehot_drop_column), nominal_categorical_features_to_encode),\n",
    "        (\"ordinal_encoder\", OrdinalEncoder(categories = ordianl_feature_categories_orders_lists), ordianl_feature_categories_to_encode)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_text(f\"Scaled Numerical Variables: {numerical_features_to_scale}\")\n",
    "display_text(f\"Encoded Nominal Categorical Variables: {nominal_categorical_features_to_encode}\")\n",
    "display_text(f\"Encoded Ordinal Categorical Variables (confirm that category orders were assigned to the correct ordinal categorical variable):\")\n",
    "if len(ordianl_feature_categories_to_encode) != 0:\n",
    "    for i in range(len(ordianl_feature_categories_to_encode)):\n",
    "        display_markdown(md(f\"* {ordianl_feature_categories_to_encode[i]}: {ordianl_feature_categories_orders_lists[i]}\"))\n",
    "else:\n",
    "    display_text(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Models and Set Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure the models you would like to use for regression.\n",
    "\"\"\"\n",
    "\n",
    "linear_model_data = {\n",
    "    'Linear': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, LinearRegression()),\n",
    "        'param_grid': {}  \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "alpha_values = np.linspace(0.01, 100, num = 100)\n",
    "\n",
    "lasso_model_data = {\n",
    "    'Lasso': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, Lasso()),\n",
    "        'param_grid': {'lasso__alpha': alpha_values}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "ridge_model_data = {\n",
    "    'Ridge': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, Ridge()),\n",
    "        'param_grid': {'ridge__alpha': alpha_values}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "random_forest_model_data = {\n",
    "    'Random Forest': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, RandomForestRegressor(random_state = 42)),\n",
    "        'param_grid': {\n",
    "            'randomforestregressor__n_estimators': [100, 200],\n",
    "            'randomforestregressor__max_depth': [None, 20],\n",
    "            'randomforestregressor__max_features': [1.0, \"sqrt\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "svr_model_data = {\n",
    "    'SVR': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, SVR()),\n",
    "        'param_grid': {\n",
    "            'svr__C': [.1, 1, 10]  \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Use the unpacking operator (**) to place all of your models in the models dictionary.\n",
    "\"\"\"\n",
    "models = {\n",
    "    **linear_model_data,\n",
    "    **lasso_model_data,\n",
    "    **ridge_model_data,\n",
    "    **random_forest_model_data,\n",
    "    **svr_model_data\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split Dataset into Training and Testing Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decimal_places: int = 7 # Determines the number of decimal places to display for model evaluation metrics\n",
    "\n",
    "# Splits dataset into feature variables and target variable\n",
    "X: pd.DataFrame = data_df.drop(columns = [target_column_name])\n",
    "y: pd.Series = data_df[target_column_name]\n",
    "num_features: int = len(X.columns)\n",
    "\n",
    "# Users can change the argument passed into the test_size parameter to adjust the size of the testing set (currently set to 20% of the dataset)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "feature_list: list[str] = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Apply Target Preprocessor**\n",
    "Do not run this cell if you are not preprocessing the target variable. This cell may need to be configured if you using something other than StandardScalaer on your target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler() requires a multidimensional array-like object as input, so the target series is converted into DataFrame so the preprocesser can be applied\n",
    "y_train: pd.DataFrame = pd.DataFrame(y_train, columns = [target_column_name])\n",
    "y_test: pd.DataFrame = pd.DataFrame(y_test, columns = [target_column_name])\n",
    "\n",
    "# Applying the StandardScaler() fit_transform method returns a 2D numpy array\n",
    "y_train: np.ndarray = numerical_target_preprocessor.fit_transform(y_train)\n",
    "y_test: np.ndarray  = numerical_target_preprocessor.fit_transform(y_test)\n",
    "\n",
    "# Must convert the target variable back to a 1D array for the models to be trained, which is done through the ravel() method\n",
    "y_train: np.ndarray = y_train.ravel()\n",
    "y_test: np.ndarray = y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Training and Evaluation Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(models_dictionary: dict[str, dict] = models,\n",
    "                              X_train: pd.DataFrame = X_train,\n",
    "                              X_test: pd.DataFrame = X_test,\n",
    "                              y_train: Union[pd.Series, np.ndarray] = y_train,\n",
    "                              y_test: Union[pd.Series, np.ndarray] = y_test) -> dict:\n",
    "    \"\"\"\n",
    "    Trains and evaluates the passed in regression models using grid search for hyperparameter tuning.\n",
    "\n",
    "    This function iterates through a dictionary of models, performs grid search cross-validation\n",
    "    for hyperparameter tuning, trains each model with the best parameters, and evaluates their\n",
    "    performance on both training and test sets. Information from this process is saved for each\n",
    "    model in the model_results dictionary to be used later for model comparison.\n",
    "\n",
    "    Args:\n",
    "        models_dict (str, dict[str, dict]): A dictionary containing model information.\n",
    "            Each key is a model name, and each value is another dictionary with 'model' and 'param_grid' keys.\n",
    "            Defaults to the global 'models' variable.\n",
    "        X_train (pd.DataFrame): The feature matrix for training.\n",
    "            Defaults to the global 'X_train' variable.\n",
    "        X_test (pd.DataFrame): The feature matrix for testing.\n",
    "            Defaults to the global 'X_test' variable.\n",
    "        y_train (Union[pd.Series, np.ndarray]): The target values for training.\n",
    "            Defaults to the global 'y_train' variable.\n",
    "        y_test (Union[pd.Series, np.ndarray]): The target values for testing.\n",
    "            Defaults to the global 'y_test_preprocessed' variable.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results for each model. Each key is a model name,\n",
    "        and each value is another dictionary with the following keys:\n",
    "            * 'best_model': The trained model with the best hyperparameters.\n",
    "            * 'best_params': The best hyperparameters found by grid search.\n",
    "            * 'best_score': The best score achieved during grid search.\n",
    "            * 'cv_results': The cross-validation results from grid search.\n",
    "            * 'tune_train_time': The time taken for hyperparameter tuning and training.\n",
    "            * 'y_train_predictions': Predictions on the training set.\n",
    "            * 'y_test_predictions': Predictions on the test set.\n",
    "            * 'train_mse': Mean Squared Error on the training set.\n",
    "            * 'test_mse': Mean Squared Error on the test set.\n",
    "            * 'test_mae': Mean Absolute Error on the test set.\n",
    "            * 'test_r2': R-squared score on the test set.\n",
    "            * 'test_adjusted_r2': Adjusted R-squared score on the test set.\n",
    "            * 'intercept': The intercept of the model (for Linear, Lasso, and Ridge models only).\n",
    "            * 'coefficients': The coefficients of the model (for Linear, Lasso, and Ridge models only).\n",
    "    \"\"\"\n",
    "    model_results = {}\n",
    "    \n",
    "    for model_name, model_data in models_dictionary.items():\n",
    "        display_text(f\"Training and evaluating: {model_name}\", font_size = 16, font_weight = 'bold')\n",
    "        \n",
    "        model = model_data['model']\n",
    "        param_grid = model_data['param_grid']\n",
    "        \n",
    "        # Users can configure the number of splits for cross-validation by changing the n_splits parameter\n",
    "        cross_validation = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv = cross_validation, scoring = 'neg_mean_squared_error')\n",
    "        tune_train_start_time = time.time()\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        tune_train_end_time = time.time()\n",
    "        tune_train_time = tune_train_end_time - tune_train_start_time\n",
    "        display_markdown(md(f\"* Hyperparameter Tuning and Model Training Time: {tune_train_time:.2f} seconds\"))\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        y_train_predictions = best_model.predict(X_train)\n",
    "        y_test_predictions = best_model.predict(X_test)\n",
    "        \n",
    "        test_r2 = r2_score(y_test, y_test_predictions)\n",
    "        train_mse = mean_squared_error(y_train, y_train_predictions)\n",
    "        test_mse = mean_squared_error(y_test, y_test_predictions)\n",
    "        test_mae = mean_absolute_error(y_test, y_test_predictions)\n",
    "        \n",
    "        num_test_entries = len(y_test)\n",
    "        test_adjusted_r2 = 1 - ((1 - test_r2) * (num_test_entries - 1) / (num_test_entries - num_features - 1))\n",
    "        \n",
    "        display_markdown(md(f\"* Test R-squared: {test_r2:.{num_decimal_places}f}\"))\n",
    "        display_markdown(md(f\"* Train MSE: {train_mse:.{num_decimal_places}f}\"))\n",
    "        display_markdown(md(f\"* Test MSE: {test_mse:.{num_decimal_places}f}\"))\n",
    "        display_markdown(md(f\"* Test MAE: {test_mae:.{num_decimal_places}f}\"))\n",
    "        display_markdown(md(f\"* Test Adjusted R^2: {test_adjusted_r2:.{num_decimal_places}f}\"))\n",
    "        \n",
    "        model_results[model_name] = {\n",
    "            'best_model': best_model,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            \"cv_results\": grid_search.cv_results_,\n",
    "            \"tune_train_time\": tune_train_time,\n",
    "            \"y_train_predictions\": y_train_predictions,\n",
    "            'y_test_predictions': y_test_predictions,\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse,\n",
    "            \"test_mae\": test_mae,\n",
    "            'test_r2': test_r2,\n",
    "            \"test_adjusted_r2\": test_adjusted_r2\n",
    "        }\n",
    "        \n",
    "        if model_name in ['Linear', 'Lasso', 'Ridge']:\n",
    "            model_results[model_name][\"intercept\"] = best_model[-1].intercept_\n",
    "            model_results[model_name][\"coefficients\"] = best_model[-1].coef_\n",
    "    \n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run Model Trianing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results: dict[str, dict] = train_and_evaluate_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Score Comparison Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparative_model_performance(model_results: dict[str, dict] = model_results) -> None:\n",
    "    \"\"\"\n",
    "    Create a comparative visualization of model performance metrics.\n",
    "\n",
    "    This function generates a plot with two subplots comparing the performance of multiple regression models.\n",
    "    The top subplot shows R-squared scores, while the bottom subplot displays Mean Squared Error (MSE)\n",
    "    for both training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        model_results (dict[str, dict]): A dictionary containing performance metrics for each model.\n",
    "            The outer dictionary keys are model names, and the inner dictionary contains the following keys:\n",
    "            * 'test_r2': R-squared score on the test set\n",
    "            * 'train_mse': Mean Squared Error on the training set\n",
    "            * 'test_mse': Mean Squared Error on the test set\n",
    "            Defaults to the global 'model_results' variable.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    models = list(model_results.keys())\n",
    "\n",
    "    # Create lists of performance metrics for plotting\n",
    "    r2_scores = [model_results[model]['test_r2'] for model in models]\n",
    "    train_mse_scores = [model_results[model]['train_mse'] for model in models]\n",
    "    test_mse_scores = [model_results[model]['test_mse'] for model in models]\n",
    "\n",
    "    # Set up the plot\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 20))\n",
    "    fig.suptitle('Comparative Model Performance', fontsize=24)\n",
    "\n",
    "    # Plot R-squared scores\n",
    "    ax1.bar(models, r2_scores, color='skyblue')\n",
    "    ax1.set_ylabel('R-squared Score')\n",
    "    ax1.set_title('R-squared Scores by Model (Higher is Better)', fontweight='bold')\n",
    "    ax1.set_xticks(np.arange(len(models)))  # Set fixed number of ticks\n",
    "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "    \n",
    "    # Adjust the y-axis to start just below the minimum R-squared score\n",
    "    r2_min = min(r2_scores) - 0.05\n",
    "    r2_max = min(max(r2_scores) + 0.05, 1.00)\n",
    "    ax1.set_ylim([r2_min, r2_max])\n",
    "\n",
    "    # Add value labels on the R-squared bars\n",
    "    for index, score in enumerate(r2_scores):\n",
    "        ax1.text(index, score, f'{score:.5f}', ha='center', va='bottom')\n",
    "\n",
    "    # Plot MSE and MAE scores\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    ax2.bar(x - (width / 2), train_mse_scores, width, label='Train MSE', color='salmon')\n",
    "    ax2.bar(x + (width / 2), test_mse_scores, width, label='Test MSE', color='lightgreen')\n",
    "    ax2.set_ylabel('Error Score')\n",
    "    ax2.set_title('MSE During Training and Testing by Model (Lower is Better)', fontweight='bold')\n",
    "    ax2.set_xticks(x)  # Set fixed number of ticks\n",
    "    ax2.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Add value labels on the MSE and MAE bars\n",
    "    for index, (train_mse, test_mse) in enumerate(zip(train_mse_scores, test_mse_scores)):\n",
    "        ax2.text(index - (width / 2), train_mse, f'{train_mse:.5f}', ha='center', va='bottom')\n",
    "        ax2.text(index + (width / 2), test_mse, f'{test_mse:.5f}', ha='center', va='bottom')\n",
    "\n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout(rect = [0, 0, 1, 0.975])  # Add space at the top for the main title\n",
    "    plt.subplots_adjust(hspace = 0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Score Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparative_model_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perfomance Summary and Model Selection Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(results: dict = model_results) -> str:\n",
    "    \"\"\"\n",
    "    This function creates a summary DataFrame of model performance metrics,\n",
    "    displays it in a formatted table, and identifies the best performing model\n",
    "    based on the R-squared score\n",
    "    \n",
    "    Args:\n",
    "        results (dict): A dictionary containing performance metrics for each model.\n",
    "            The keys are model names, and the values are dictionaries containing:\n",
    "            * 'test_r2': R-squared score on the test set\n",
    "            * 'train_mse': Mean Squared Error on the training set\n",
    "            * 'test_mse': Mean Squared Error on the test set\n",
    "            * 'test_mae': Mean Absolute Error on the test set\n",
    "            * 'test_adjusted_r2': Adjusted R-squared score on the test set\n",
    "            Defaults to the global 'model_results' variable.\n",
    "    Returns:\n",
    "        best_performing_model_name (str): The name of the best performing model based on the highest R-squared score.\n",
    "    \"\"\" \n",
    "    summary_df = pd.DataFrame({\n",
    "        'Model': results.keys(),\n",
    "        'Test R-squared': [result['test_r2'] for result in results.values()],\n",
    "        \"Train MSE\": [result['train_mse'] for result in results.values()],\n",
    "        'Test MSE': [result['test_mse'] for result in results.values()],\n",
    "        'Test MAE': [result['test_mae'] for result in results.values()],\n",
    "        'Test Adjusted R-squared': [result['test_adjusted_r2'] for result in results.values()]\n",
    "    })\n",
    "\n",
    "    display_text(\"Model Performance Summary\", font_size = 20, font_weight = \"bold\")\n",
    "    \n",
    "    display_dataframe(summary_df)\n",
    "    print()\n",
    "\n",
    "    best_performing_model_name = summary_df.loc[summary_df['Test R-squared'].idxmax(), 'Model']\n",
    "    display_markdown(md(f\"### **Best Performing Model: {best_performing_model_name}** (based on R-squared score, the higher the better)\"))\n",
    "    display_markdown(md(f\"* #### R-squared Score: {results[best_performing_model_name]['test_r2']}\"))\n",
    "\n",
    "    return best_performing_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Performance Summary and Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfomrmance summary function saves the name of the best performing model\n",
    "best_model_name: str = summarize_results()\n",
    "best_model = model_results[best_model_name]['best_model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparative Plotting Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals_histograms_comparison(model_results: dict[str, dict], y_test: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Plot histograms of residuals for multiple models vertically.\n",
    "    \n",
    "    Args:\n",
    "        model_results (dict[str, dict]): Dictionary of model results, where each key is a model name\n",
    "        and each value is a dictionary containing model predictions\n",
    "        y_test (np.ndarray): Actual target values\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    num_models = len(model_results)\n",
    "    num_rows = math.ceil(num_models / 2)\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(20, 7*num_rows))\n",
    "    \n",
    "    fig.suptitle('Comparison of Residuals Distributions', fontsize=24, y=1.02)\n",
    "    \n",
    "    axes_flat = axes.flatten() if num_models > 1 else [axes]\n",
    "    \n",
    "    # Determine global x-axis limits\n",
    "    all_residuals = []\n",
    "    for results in model_results.values():\n",
    "        all_residuals.extend(y_test - results['y_test_predictions'])\n",
    "    global_xlim = np.percentile(all_residuals, [1, 99])\n",
    "    \n",
    "    error_stats = {}\n",
    "    for ax, (model_name, results) in zip(axes_flat, model_results.items()):\n",
    "        y_pred = results['y_test_predictions']\n",
    "        residuals = y_test - y_pred\n",
    "        \n",
    "        # Plot histogram and KDE\n",
    "        sns.histplot(residuals, kde=True, ax=ax, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        \n",
    "        ax.set_title(f'{model_name} Residuals', fontsize=18, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Residuals (Actual - Predicted)', fontsize=14)\n",
    "        ax.set_ylabel('Density', fontsize=14)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        \n",
    "        ax.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
    "        ax.set_xlim(global_xlim)\n",
    "        \n",
    "        # Calculate and display statistics\n",
    "        mean_residual = np.mean(residuals)\n",
    "        median_residual = np.median(residuals)\n",
    "        std_residual = np.std(residuals)\n",
    "        \n",
    "        stats_text = (f'Mean: {mean_residual:.3f}\\n'\n",
    "                      f'Median: {median_residual:.3f}\\n'\n",
    "                      f'Std Dev: {std_residual:.3f}')\n",
    "        \n",
    "        ax.text(0.95, 0.95, stats_text, transform=ax.transAxes, \n",
    "                verticalalignment='top', horizontalalignment='right',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                fontsize=12)\n",
    "        \n",
    "        ax.grid(True, linestyle=':', alpha=0.6)\n",
    "        ax.legend(loc='upper left')\n",
    "        \n",
    "        # Calculate and save statistics for percentage errors\n",
    "        percentage_errors = (residuals / y_test) * 100\n",
    "        mean_error = np.mean(percentage_errors)\n",
    "        median_error = np.median(percentage_errors)\n",
    "        std_error = np.std(percentage_errors)\n",
    "\n",
    "        error_stats[model_name] = {\n",
    "            'mean_error': mean_error,\n",
    "            'median_error': median_error,\n",
    "            'std_error': std_error\n",
    "        }\n",
    "    \n",
    "    for ax in axes_flat[num_models:]:\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.96, hspace=0.3, wspace=0.2)\n",
    "    plt.show()\n",
    "    return error_stats\n",
    "\n",
    "\n",
    "def plot_residuals_scatter_comparison(model_results: dict[str, dict], y_test: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    This function generates a grid of scatter plots, one for each model in the input dictionary.\n",
    "    Each plot visualizes the relationship between predicted values and their residuals,\n",
    "    helping users better understand model performance and compare different models.\n",
    "\n",
    "    Args:\n",
    "        model_results (dict[str, dict]): A dictionary where each key is a model name and each value\n",
    "            is another dictionary containing at least the key 'y_test_predictions', a numpy array\n",
    "            of model predictions\n",
    "        y_test (np.ndarray): Array of actual target values from the test set\n",
    "    \"\"\"\n",
    "    num_models = len(model_results)\n",
    "    num_rows = math.ceil(num_models / 2)\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(22, 9*num_rows))  # Increased width for colorbars\n",
    "    \n",
    "    fig.suptitle('Comparison of Residuals Scatter Plots', fontsize=24, y=1.02)\n",
    "    \n",
    "    axes_flat = axes.flatten() if num_models > 1 else [axes]\n",
    "    \n",
    "    # Determine global axis limits and density range\n",
    "    all_predicted = []\n",
    "    all_residuals = []\n",
    "    all_densities = []\n",
    "    for results in model_results.values():\n",
    "        y_pred = results['y_test_predictions']\n",
    "        residuals = y_test - y_pred\n",
    "        all_predicted.extend(y_pred)\n",
    "        all_residuals.extend(residuals)\n",
    "        xy = np.vstack([y_pred, residuals])\n",
    "        all_densities.extend(stats.gaussian_kde(xy)(xy))\n",
    "    \n",
    "    global_xlim = np.percentile(all_predicted, [1, 99])\n",
    "    global_ylim = np.percentile(all_residuals, [1, 99])\n",
    "    vmin, vmax = np.percentile(all_densities, [5, 95])  # For consistent colorbar scale\n",
    "    \n",
    "    for ax, (model_name, results) in zip(axes_flat, model_results.items()):\n",
    "        y_pred = results['y_test_predictions']\n",
    "        residuals = y_test - y_pred\n",
    "        \n",
    "        # Create scatter plot with density coloring\n",
    "        xy = np.vstack([y_pred, residuals])\n",
    "        density = stats.gaussian_kde(xy)(xy)\n",
    "        \n",
    "        idx = density.argsort()\n",
    "        x, y, z = y_pred[idx], residuals[idx], density[idx]\n",
    "        \n",
    "        scatter = ax.scatter(x, y, c=z, cmap='viridis', alpha=0.7, vmin=vmin, vmax=vmax)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(y_pred, residuals, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax.plot(y_pred, p(y_pred), \"black\", alpha=0.8, label='Trend') \n",
    "        \n",
    "        ax.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Perfect Prediction')  \n",
    "        ax.set_xlim(global_xlim)\n",
    "        ax.set_ylim(global_ylim)\n",
    "        \n",
    "        ax.set_title(f'{model_name} Residuals', fontsize=18, fontweight='bold', pad=20)\n",
    "        ax.set_xlabel('Predicted Values', fontsize=14)\n",
    "        ax.set_ylabel('Residuals', fontsize=14)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        \n",
    "        ax.grid(True, linestyle=':', alpha=0.6)\n",
    "        ax.legend(loc='lower right')\n",
    "        \n",
    "        # Add individual colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=ax, pad=0.02)\n",
    "        cbar.set_label('Density', fontsize=12)\n",
    "        cbar.ax.tick_params(labelsize=10)\n",
    "    \n",
    "    for ax in axes_flat[num_models:]:\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.97, hspace=0.2, wspace=0.1)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_actual_vs_predicted_comparison(model_results: dict[str, dict], y_test: np.ndarray, target_name: str = \"Target\") -> None:\n",
    "    \"\"\"\n",
    "    Plots enhanced actual vs predicted values comparison for multiple regression models with improved range.\n",
    "    \n",
    "    Args:\n",
    "    model_results (dict[str, dict]): Dictionary of model results, where each key is a model name\n",
    "        and each value is a dictionary containing model predictions.\n",
    "    y_test (np.ndarray): Actual target values.\n",
    "    target_name (str): Name of the target variable for axis labels.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    num_models = len(model_results)\n",
    "    num_rows = math.ceil(num_models / 2)\n",
    "    fig, axes = plt.subplots(num_rows, 2, figsize=(20, 10*num_rows))\n",
    "    fig.suptitle(f'Comparison of Actual vs Predicted {target_name} Values', fontsize=26, y=1.02)\n",
    "    \n",
    "    # Flatten axes array for easy iteration\n",
    "    axes_flat = axes.flatten() if num_models > 2 else ([axes] if num_models == 1 else axes)\n",
    "    \n",
    "    # Calculate global min and max for consistent scaling\n",
    "    all_y = np.concatenate([y_test] + [results['y_test_predictions'] for results in model_results.values()])\n",
    "    global_min, global_max = np.min(all_y), np.max(all_y)\n",
    "    \n",
    "    # Calculate the range and add a buffer (e.g., 10% of the range on each side)\n",
    "    data_range = global_max - global_min\n",
    "    buffer = 0.1 * data_range\n",
    "    plot_min = global_min - buffer\n",
    "    plot_max = global_max + buffer\n",
    "    \n",
    "    # Ensure the aspect ratio is equal and adjust the limits if necessary\n",
    "    center = (plot_min + plot_max) / 2\n",
    "    half_range = max(plot_max - center, center - plot_min)\n",
    "    plot_min = center - half_range\n",
    "    plot_max = center + half_range\n",
    "    \n",
    "    # Calculate global min and max errors\n",
    "    all_errors = np.concatenate([np.abs(y_test - results['y_test_predictions']) for results in model_results.values()])\n",
    "    vmin, vmax = np.min(all_errors), np.max(all_errors)\n",
    "    \n",
    "    for ax, (model_name, results) in zip(axes_flat, model_results.items()):\n",
    "        y_pred = results['y_test_predictions']\n",
    "        errors = np.abs(y_test - y_pred)\n",
    "        \n",
    "        # Set background color to white and add grid\n",
    "        ax.set_facecolor('white')\n",
    "        ax.grid(True, linestyle=':', alpha=0.5, color='gray')\n",
    "        \n",
    "        # Create scatter plot with smaller, more transparent points\n",
    "        scatter = ax.scatter(y_test, y_pred, c=errors, cmap='viridis', alpha=0.7, s=20, vmin=vmin, vmax=vmax)\n",
    "        \n",
    "        # Add diagonal line for perfect predictions\n",
    "        ax.plot([plot_min, plot_max], [plot_min, plot_max], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "        \n",
    "        # Set consistent scale with balanced range\n",
    "        ax.set_xlim(plot_min, plot_max)\n",
    "        ax.set_ylim(plot_min, plot_max)\n",
    "        \n",
    "        # Adjust title and labels\n",
    "        ax.set_title(model_name, fontsize=20, fontweight='bold', pad=20)\n",
    "        ax.set_ylabel(f'Predicted {target_name}', fontsize=14)\n",
    "        ax.set_xlabel(f'Actual {target_name}', fontsize=14)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        \n",
    "        # Add legend for perfect prediction line\n",
    "        ax.legend(fontsize=10, loc='lower right')\n",
    "        \n",
    "        # Add colorbar for each subplot\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "        cbar = plt.colorbar(scatter, cax=cax)\n",
    "        cbar.set_label('Absolute Prediction Error Gradient', fontsize=12)\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for ax in axes_flat[num_models:]:\n",
    "        ax.set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top = .97, hspace=0.2, wspace=0.25)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparative Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_histograms_comparison(model_results, y_test)\n",
    "plot_residuals_scatter_comparison(model_results, y_test)\n",
    "plot_actual_vs_predicted_comparison(model_results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Individual Model Analysis**\n",
    "Use this section to get a better view of the plots of models you are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Individual Model Performance Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fit_function_equation_markdown(intercept: int, coefficient_list: list, feature_list: list = feature_list, num_decimal_places: int = num_decimal_places) -> str:\n",
    "    model_equation = f\"### {num_decimal_places} = {intercept:.{num_decimal_places}f}\"\n",
    "    \n",
    "    for i in range(len(coefficient_list)):\n",
    "        model_equation += f\" + {coefficient_list[i]:.{num_decimal_places}f}({feature_list[i]})\"\n",
    "        \n",
    "    return model_equation\n",
    "\n",
    "def display_model_evaluation_results(model_name: str, results: dict = model_results) -> None:\n",
    "    \"\"\"\n",
    "    Displays the evaluation results for a given model.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model\n",
    "        results (dict): Dictionary containing the evalutation results of each model\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    display_markdown(md(f\"### **Model: {model_name}**\"))\n",
    "    display_markdown(md(f\"* #### **Train MSE:** {results[model_name][\"train_mse\"]:.{num_decimal_places}f}\"))\n",
    "    display_markdown(md(f\"* #### **Test MSE:** {results[model_name][\"test_mse\"]:.{num_decimal_places}f}\"))\n",
    "    display_markdown(md(f\"* #### **Test MAE:** {results[model_name][\"test_mae\"]:.{num_decimal_places}f}\"))\n",
    "    display_markdown(md(f\"* #### **Test R^2:** {results[model_name][\"test_r2\"]:.{num_decimal_places}f}\"))\n",
    "    display_markdown(md(f\"* #### **Test Adjusted R^2:** {results[model_name][\"test_adjusted_r2\"]:.{num_decimal_places}f}\"))\n",
    "    \n",
    "    if model_name in ['Linear', 'Lasso', 'Ridge']:\n",
    "        display_markdown(md(f\"### **Model Equation:**\"))\n",
    "        display_markdown(md(create_fit_function_equation_markdown(results[model_name][\"intercept\"], results[model_name][\"coefficients\"])))\n",
    "        \n",
    "    \n",
    "def plot_residuals_histogram(model_name: str, residuals: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Display a histogram of residuals with KDE and statistics.\n",
    "    \n",
    "    Args:\n",
    "        residuals (np.ndarray): Residuals (actual - predicted)\n",
    "        model_name (str): Name of the model\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(residuals, kde=True, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Residuals (Actual - Predicted)')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Histogram of Residuals ({model_name})')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    mean_residual = np.mean(residuals)\n",
    "    median_residual = np.median(residuals)\n",
    "    std_residual = np.std(residuals)\n",
    "\n",
    "    stats_text = (f'Mean: {mean_residual:.3f}\\n'\n",
    "                  f'Median: {median_residual:.3f}\\n'\n",
    "                  f'Std Dev: {std_residual:.3f}\\n')\n",
    "    \n",
    "    plt.text(0.95, 0.95, stats_text, transform=plt.gca().transAxes, \n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "             fontsize=12)\n",
    "    \n",
    "    plt.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_residuals_scatter(model_name: str, y_pred: np.ndarray, residuals: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Displays a scatter plot of residuals vs predicted values with density coloring.\n",
    "    \n",
    "    Args:\n",
    "        y_pred (np.ndarray): Predicted target values\n",
    "        residuals (np.ndarray): Residuals (actual - predicted)\n",
    "        model_name (str): Name of the model\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate density for coloring\n",
    "    xy = np.vstack([y_pred, residuals])\n",
    "    density = stats.gaussian_kde(xy)(xy)\n",
    "    \n",
    "    idx = density.argsort()\n",
    "    x, y, z = y_pred[idx], residuals[idx], density[idx]\n",
    "    \n",
    "    scatter = plt.scatter(x, y, c=z, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(y_pred, residuals, 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(y_pred, p(y_pred), \"black\", alpha=0.8, label='Trend')\n",
    "\n",
    "    plt.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
    "    plt.xlabel('Predicted Target Values')\n",
    "    plt.ylabel('Residuals (Actual - Predicted)')\n",
    "    plt.title(f'Residual Plot ({model_name})')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Density', fontsize=12)\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_actual_vs_predicted(model_name: str, y_test: np.ndarray, y_pred: np.ndarray, target_name: str = \"Target Values\") -> None:\n",
    "    \"\"\"\n",
    "    Displays a plot of actual vs predicted values with error gradient.\n",
    "    \n",
    "    Args:\n",
    "        y_test (np.ndarray): Actual target values\n",
    "        y_pred (np.ndarray): Predicted target values\n",
    "        model_name (str): Name of the model\n",
    "        target_name (str): Name of the target variable for axis labels\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    errors = np.abs(y_test - y_pred)\n",
    "    plt.figure(figsize=(13, 6))\n",
    "    scatter = plt.scatter(y_test, y_pred, c=errors, cmap='viridis', alpha=0.7)\n",
    "    plt.colorbar(scatter, label='Absolute Error')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "    plt.xlabel(f'Actual {target_name}')\n",
    "    plt.ylabel(f'Predicted {target_name}')\n",
    "    plt.title(f'Actual vs. Predicted with Error Gradient ({model_name})')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Move legend to the bottom right\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def display_model_evaluation_and_plots(model_name: str, y_test: np.ndarray = y_test) -> None:\n",
    "    \"\"\"\n",
    "    Display the evaluation results and plots for a given model.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    y_test_predictions = model_results[model_name][\"y_test_predictions\"]\n",
    "    residuals = y_test - y_test_predictions\n",
    "\n",
    "    display_model_evaluation_results(model_name)\n",
    "    plot_residuals_histogram(model_name, residuals)\n",
    "    plot_residuals_scatter(model_name, y_test_predictions, residuals)\n",
    "    plot_actual_vs_predicted(model_name, y_test, y_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_name: str = \"Linear\"\n",
    "\n",
    "display_model_evaluation_and_plots(linear_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lasso Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model_name: str = \"Lasso\"\n",
    "\n",
    "display_model_evaluation_and_plots(lasso_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ridge Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model_name: str = \"Ridge\"\n",
    "\n",
    "display_model_evaluation_and_plots(ridge_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model_name: str = \"Random Forest\"\n",
    "\n",
    "display_model_evaluation_and_plots(random_forest_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Support Vector Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model_name: str = \"SVR\"\n",
    "\n",
    "display_model_evaluation_and_plots(svr_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure the name of the model you want to save (best_model_name by default), the name of the file\n",
    "that the model should be saved to, and the save method.\n",
    "\"\"\"\n",
    "name_of_model_to_save = best_model_name\n",
    "save_file_name: str = \"\"\n",
    "save_method: str = \"\" # Options: \"pickle\" or \"joblib\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Functions for Saving Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_pickle(file_name: str = save_file_name,\n",
    "                      model_name: str = name_of_model_to_save,\n",
    "                      results: dict[str, dict] = model_results) -> None:\n",
    "    \"\"\"\n",
    "    Saves the best performing model to a pickle file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Name of the pickle file to save the model to\n",
    "        model_name (str): Name of the model that is about to be saved\n",
    "        results (dict[str, dict]): Dictionary containing the model object, predictions on the testing data, and other model perfomance data\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model_to_save = results[model_name]['best_model']\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(model_to_save, file)\n",
    "        \n",
    "\n",
    "def save_model_joblib(file_name: str = save_file_name,\n",
    "                      model_name: str = name_of_model_to_save,\n",
    "                      results: dict[str, dict] = model_results) -> None:\n",
    "    \"\"\"\n",
    "    Saves the best performing model to a joblib file.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Name of the joblib file to save the model to\n",
    "        model_name (str): Name of the model that is about to be saved\n",
    "        results (dict[str, dict]): Dictionary containing the model object, predictions on the testing data, and other model perfomance data\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model_to_save = results[model_name]['best_model']\n",
    "    dump(model_to_save, file_name)\n",
    "    \n",
    "\n",
    "def save_model(file_name: str = save_file_name,\n",
    "               model_name: str = name_of_model_to_save,\n",
    "               results: dict[str, dict] = model_results,\n",
    "               method: str = save_method) -> None:\n",
    "    \"\"\"\n",
    "    Saves the best performing model to a file using the specified method.\n",
    "    \n",
    "    Args:\n",
    "        file_name (str): Name of the file to save the model to\n",
    "        model_name (str): Name of the model to save\n",
    "        results (dict[str, dict]): Dictionary containing the model object, predictions on the testing data, and other model perfomance data\n",
    "        method (str): Method to use for saving the model (\"pickle\" or \"joblib\")\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if method == \"pickle\":\n",
    "        save_model_pickle(file_name, model_name, results)\n",
    "    elif method == \"joblib\":\n",
    "        save_model_joblib(file_name, model_name, results)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method specified. Please use 'pickle' or 'joblib'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regression-template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
