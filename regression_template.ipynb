{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary to display the plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ydata_profiling import ProfileReport\n",
    "from IPython.display import display_html, display_markdown, HTML, Markdown as md\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.lines import Line2D\n",
    "import math\n",
    "import re\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from joblib import dump\n",
    "import time\n",
    "from typing import Union\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import KFold,GridSearchCV, train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Helper functions defined in the helper.py file\n",
    "import helper as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&display=swap');\n",
       "div.text_cell {\n",
       "    font-family : DM Sans, sans-serif !important;\n",
       "    font-size : 1.2em !important;\n",
       "}\n",
       "pre {font-family : DM Sans, sans-serif !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "@import url('https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&display=swap');\n",
    "div.text_cell {\n",
    "    font-family : DM Sans, sans-serif !important;\n",
    "    font-size : 1.2em !important;\n",
    "}\n",
    "pre {font-family : DM Sans, sans-serif !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regression Model Comparison Template** \n",
    "This Notebook compares the performance of different types of regression models on a dataset provided by the user. It assists in the model selection process by streamlining data cleaning, data preprocessing, model training, and model evaluation. Throughout the template there are sections the user must configure to match characteristics of their dataset. These sections are preceeded by tripple quote comments that direct them on how to proceed. Users should store files containing their data in the data directory. The load_data function automatically prepends /data to the given file name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load and Reformat the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configure Data Loading Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input the name of the file containing your dataset, a list of the name of columns to drop, and a list of\n",
    "the representations of missing values in the dataset.\n",
    "\"\"\"\n",
    "data_file_name: str = \"(ex: data.csv)\" \n",
    "columns_to_drop: list[str] = [] \n",
    "\n",
    "# Ensure no valid dataset values are included in this list\n",
    "dataset_na_value_representations = ['', 'NA', 'N/A', 'null', 'NULL', 'NaN', 'none', 'None', '-', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df: pd.DataFrame = hp.load_data(data_file_name, columns_to_drop, dataset_na_value_representations)\n",
    "\n",
    "initial_number_of_entries: int = len(data_df) \n",
    "variable_list: list[str] = list(data_df.columns)\n",
    "number_of_variables: int = len(variable_list)\n",
    "\n",
    "numerical_variables: list[str] = list(data_df.select_dtypes(include = np.number).columns)\n",
    "categorical_variables: list[str] = list(data_df.select_dtypes(exclude = np.number).columns)\n",
    "\n",
    "hp.display_text(f\"Numerical Variables: {numerical_variables}\", font_size = 16)\n",
    "hp.display_text(f\"Categorical Variables: {categorical_variables}\", font_size = 16)\n",
    "\n",
    "hp.display_df(data_df.head(), font_size = 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reformat Columns**\n",
    "Do not run this cell if the values of your dataset are already properly formatted. If not (e.g. columns that should be numerical are instead represented as strings), open the cell and configure this section to reformat any imporperly formatted columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply value cleaning/conversion functions to the relevant columns of your datset using the apply \n",
    "method on your DataFrame. Your cleaing/conversion functions should take in a single string and return a float.\n",
    "Pass them into the apply method as an argument without parentheses.\n",
    "Example: data_df['column_name'] = data_df['column_name'].apply(clean_function).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "hp.display_text(f\"Previous List of Numerical Variables: {numerical_variables}\", font_size = 18)\n",
    "hp.display_text(f\"Previous List of Categorical Variables: {categorical_variables}\", font_size = 18)\n",
    "print()\n",
    "\n",
    "numerical_variables: list[str] = list(data_df.select_dtypes(include = np.number).columns)\n",
    "categorical_variables: list[str] = list(data_df.select_dtypes(exclude = np.number).columns)\n",
    "\n",
    "hp.display_text(f\"Updated List of Numerical Variables: {numerical_variables}\", font_size = 18)\n",
    "hp.display_text(f\"Updated List of Categorical Variables: {categorical_variables}\", font_size = 18)\n",
    "\n",
    "hp.display_df(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handle Missing Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initial Data Profiling**\n",
    "The following cell uses ydata-profiling to generate a detailed report on the characteristics of the input dataset. Use this information to help you determine how to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[categorical_variables] = data_df[categorical_variables].astype(\"category\")\n",
    "\n",
    "initial_dataset_report = ProfileReport(data_df, title = \"Dataset Profiling Report (Before Handling Outliers/Missing Values)\", progress_bar = False, explorative = True)\n",
    "initial_dataset_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Information on Missing Values in Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_with_missing_values: list[str] = data_df[numerical_variables].columns[data_df[numerical_variables].isnull().any()].tolist()\n",
    "categorical_columns_with_missing_values: list[str] = data_df[categorical_variables].columns[data_df[categorical_variables].isnull().any()].tolist()\n",
    "all_columns_with_missing_values: list[str] = numerical_columns_with_missing_values + categorical_columns_with_missing_values\n",
    "\n",
    "if len(all_columns_with_missing_values) != 0:\n",
    "    print()\n",
    "    entries_with_missing_values_df: pd.DataFrame = data_df[all_columns_with_missing_values][data_df[all_columns_with_missing_values].isnull().any(axis = \"columns\")]\n",
    "    number_of_entries_with_missing_values: int = len(entries_with_missing_values_df)\n",
    "    percent_of_entries_with_missing_values: float = (number_of_entries_with_missing_values / initial_number_of_entries) * 100  \n",
    "    \n",
    "    hp.display_text(f\"Total Number of Entries: {initial_number_of_entries}\")\n",
    "    hp.display_text(f\"Total Number of Entreis with at Least One Missing Value: {number_of_entries_with_missing_values} ({percent_of_entries_with_missing_values:.2f}% of Entries)\")\n",
    "    hp.display_text(f\"Number of Entries if all Rows with Missing Values are Dropped: {initial_number_of_entries - number_of_entries_with_missing_values}\")\n",
    "    print()\n",
    "    hp.display_text(\"Up to First 5 Entries with Missing Values:\")\n",
    "    hp.display_df(entries_with_missing_values_df.head(), font_size = 16)\n",
    "else:\n",
    "    print()\n",
    "    hp.display_text(\"No Missing Values in Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Drop or Impute Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Specify how you would like to handle missing values in the dataset. All rows with missing data are dropped by default. Will it work\n",
    "\"\"\"\n",
    "data_df = hp.drop_rows_with_missing_values(data_df, all_columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handle Outliers/Eronious Entries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implement Outlier Handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides information on variable distributions to help users determine whether they should drop outlier entries\n",
    "hp.display_text(\"Previous Numerical Variable Statistics\", font_size = 20)\n",
    "hp.display_df (data_df.describe(), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use the visualize_outliers function to identify and optionally remove outliers in the numerical columns of your dataset.\n",
    "\"\"\"\n",
    "data_df = hp.visualize_outliers(data_df, numerical_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.display_text(\"Updated Numerical Variable Statistics\", font_size = 20)\n",
    "hp.display_df(data_df.describe(), 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**\n",
    "Define your preprocessing steps within this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Preprocessing Information**\n",
    "Use the information provided by the comparison profile report to analyze the result of your data cleaning and to help determine your preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_report = ProfileReport(data_df, title = \"Dataset Profiling Report (After Handling Outliers/Missing Values)\", progress_bar = False, explorative = True)\n",
    "# The difference between the profiling report before and after preprocessing may not be very significant depending on the number of missing values\n",
    "# and removed outliers\n",
    "post_cleaning_comparison_report = dataset_report.compare(initial_dataset_report)\n",
    "\n",
    "#post_cleaning_comparison_report\n",
    "dataset_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configure Preprocessing Steps for the Provided General Preprocessor**\n",
    "The provided general preprocessor address three common preprocessing transformations: scaling numerical variables, one-hot encoding nominal categorical variables, and ordianal encoding ordinal categorical variables. Customize these steps to fit the needs of your dataset. Preprocessing for your target variable and feature variables must be handled by seperate transformer variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input the name of the target variable column, a list of the numerical features you would like to scale (defaultes to all numerical\n",
    "features in the dataset), and a list of the nominal categorical features you would like to one-hot encode.\n",
    "\"\"\"\n",
    "target_column_name: str = \"\"\n",
    "numerical_features_to_scale: list[str] = list(set(numerical_variables) - set([target_column_name]))\n",
    "nominal_categorical_features_to_encode: list[str] = []\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The ordianl_categories_ordered_dict variable represents the order of ordinal categorical variable categories in a dictionary.\n",
    "For the keys of this dictionary, input the column names of ordinal categorical variables. Each key's value should be a lists\n",
    "of variable categories ordered from \"smallest\" to \"largest\". Example:\n",
    "ordianl_categories_ordered_dict = {\n",
    "    \"size\": [\"small\", \"medium\", \"large\"],\n",
    "    \"grade\": [\"F\", \"D\", \"C\", \"B\", \"A]\n",
    "}\n",
    "\"\"\"\n",
    "ordianl_categories_ordered_dict: dict[str, list[str]] = {}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Specify the preprocessor used on the target variable column. Default is StandardScaler.\n",
    "\"\"\"\n",
    "numerical_target_preprocessor = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **General Preprocessor Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the keys and values of the ordianl_categories_ordered_dict into separate lists\n",
    "ordianl_feature_categories_to_encode: list[str] = list(ordianl_categories_ordered_dict.keys())\n",
    "ordianl_feature_categories_orders_lists: list[list[str]] = list(ordianl_categories_ordered_dict.values())\n",
    "\n",
    "# Indicates that the first column of one-hot encoded variables should be dropped to avoid multicollinearity\n",
    "onehot_drop_column: str = \"first\"\n",
    "\n",
    "# The argumnet for the transformers parameter of ColumnTransformer must be a a list of touples with three entries. Each of these touples\n",
    "# represents a preprocessing step. The first entry of each touple is a name for the step. The second entry is the transformer object, and\n",
    "# the final entry is a list of the columns the step should be applied to.\n",
    "general_feature_preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('numerical_scaler', StandardScaler(), numerical_features_to_scale),\n",
    "        ('nominal_encoder', OneHotEncoder(drop = onehot_drop_column), nominal_categorical_features_to_encode),\n",
    "        (\"ordinal_encoder\", OrdinalEncoder(categories = ordianl_feature_categories_orders_lists), ordianl_feature_categories_to_encode)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preprocessing Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.display_text(f\"Scaled Numerical Variables: {numerical_features_to_scale}\")\n",
    "hp.display_text(f\"Encoded Nominal Categorical Variables: {nominal_categorical_features_to_encode}\")\n",
    "hp.display_text(f\"Encoded Ordinal Categorical Variables (confirm that category orders were assigned to the correct ordinal categorical variable):\")\n",
    "if len(ordianl_feature_categories_to_encode) != 0:\n",
    "    for i in range(len(ordianl_feature_categories_to_encode)):\n",
    "        display_markdown(md(f\"* {ordianl_feature_categories_to_encode[i]}: {ordianl_feature_categories_orders_lists[i]}\"))\n",
    "else:\n",
    "    hp.display_text(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load Models and Set Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure the models you would like to use for regression.\n",
    "\"\"\"\n",
    "\n",
    "linear_model_data = {\n",
    "    'Linear': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, LinearRegression()),\n",
    "        'param_grid': {}  \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "alpha_values = np.linspace(0.01, 100, num = 100)\n",
    "\n",
    "lasso_model_data = {\n",
    "    'Lasso': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, Lasso()),\n",
    "        'param_grid': {'lasso__alpha': alpha_values}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "ridge_model_data = {\n",
    "    'Ridge': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, Ridge()),\n",
    "        'param_grid': {'ridge__alpha': alpha_values}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "random_forest_model_data = {\n",
    "    'Random Forest': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, RandomForestRegressor(random_state = 42)),\n",
    "        'param_grid': {\n",
    "            'randomforestregressor__n_estimators': [100, 200],\n",
    "            'randomforestregressor__max_depth': [None, 20],\n",
    "            'randomforestregressor__max_features': [1.0, \"sqrt\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "svr_model_data = {\n",
    "    'SVR': {\n",
    "        'model': make_pipeline(general_feature_preprocessor, SVR()),\n",
    "        'param_grid': {\n",
    "            'svr__C': [.1, 1, 10]  \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Use the unpacking operator (**) to place all of your models in the models dictionary.\n",
    "\"\"\"\n",
    "models = {\n",
    "    **linear_model_data,\n",
    "    **lasso_model_data,\n",
    "    **ridge_model_data,\n",
    "    **random_forest_model_data,\n",
    "    **svr_model_data\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split Dataset into Training and Testing Sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decimal_places: int = 7 # Determines the number of decimal places to display for model evaluation metrics\n",
    "\n",
    "# Splits dataset into feature variables and target variable\n",
    "X: pd.DataFrame = data_df.drop(columns = [target_column_name])\n",
    "y: pd.Series = data_df[target_column_name]\n",
    "num_features: int = len(X.columns)\n",
    "\n",
    "# Users can change the argument passed into the test_size parameter to adjust the size of the testing set (currently set to 20% of the dataset)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "feature_list: list[str] = list(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Apply Target Preprocessor**\n",
    "Do not run this cell if you are not preprocessing the target variable. This cell may need to be configured if you using something other than StandardScalaer on your target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler() requires a multidimensional array-like object as input, so the target series is converted into DataFrame so the preprocesser can be applied\n",
    "y_train: pd.DataFrame = pd.DataFrame(y_train, columns = [target_column_name])\n",
    "y_test: pd.DataFrame = pd.DataFrame(y_test, columns = [target_column_name])\n",
    "\n",
    "# Applying the StandardScaler() fit_transform method returns a 2D numpy array\n",
    "y_train: np.ndarray = numerical_target_preprocessor.fit_transform(y_train)\n",
    "y_test: np.ndarray  = numerical_target_preprocessor.fit_transform(y_test)\n",
    "\n",
    "# Must convert the target variable back to a 1D array for the models to be trained, which is done through the ravel() method\n",
    "y_train: np.ndarray = y_train.ravel()\n",
    "y_test: np.ndarray = y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run Model Trianing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results: dict[str, dict] = hp.train_and_evaluate_models(models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Score Comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.plot_comparative_model_performance(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Performance Summary and Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfomrmance summary function saves the name of the best performing model\n",
    "best_model_name: str = hp.summarize_results(model_results)\n",
    "best_model = model_results[best_model_name]['best_model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparative Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.plot_residuals_histograms_comparison(model_results, y_test)\n",
    "hp.plot_residuals_scatter_comparison(model_results, y_test)\n",
    "hp.plot_actual_vs_predicted_comparison(model_results, y_test, target_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Individual Model Analysis**\n",
    "Use this section to get a better view of the plots of models you are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_name: str = \"Linear\"\n",
    "\n",
    "display_model_evaluation_and_plots(linear_model_name, model_results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lasso Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model_name: str = \"Lasso\"\n",
    "\n",
    "display_model_evaluation_and_plots(lasso_model_name, model_results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ridge Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model_name: str = \"Ridge\"\n",
    "\n",
    "display_model_evaluation_and_plots(ridge_model_name, model_results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model_name: str = \"Random Forest\"\n",
    "\n",
    "display_model_evaluation_and_plots(random_forest_model_name, model_results, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Support Vector Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model_name: str = \"SVR\"\n",
    "\n",
    "display_model_evaluation_and_plots(svr_model_name), model_results, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Save Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure the name of the model you want to save (best_model_name by default), the name of the file\n",
    "that the model should be saved to, and the save method.\n",
    "\"\"\"\n",
    "name_of_model_to_save = best_model_name\n",
    "save_file_name: str = \"\"\n",
    "save_method: str = \"\" # Options: \"pickle\" or \"joblib\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.save_model(name_of_model_to_save, best_model, save_file_name, save_method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regression-template",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
